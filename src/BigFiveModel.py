import torch
import pyro
import pyro.distributions as dist
from .common import N_COUNTIRES, load_joint, empty_df, positive_correlation, QUESTIONS, NEGATIVE_QUESTIONS
from .BaseModel import BaseModel
from sklearn.preprocessing import OrdinalEncoder
import torch.distributions.constraints as const
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import seaborn as sns
import pandas as pd
from IPython.display import display
import pyro.poutine as poutine
from scipy import stats

class BigFiveModel(BaseModel):
    """
    Bayesian interpretation of BigFive model.
    """

    # Parameters of prior trait distributions
    ALPHA_PRIOR = 1.0
    BETA_PRIOR = 1.0
    
    def __init__(self):           
        super().__init__(None)
    
    def model(self, n_samples=None, anwser_mask=True):
        """p(x) for BigFiveModel

        Keyword Arguments:
            n_samples {int} -- Number of samples to generate (default: {None})
            anwser_mask {bool or list of booleans} -- Mask with shape of anwsers. 1.0 represent observed value and False represent unobserved (default: {True})

        Returns:
            (anwser, trait) -- Anwsers and traits that were generated by the model
        """
        if n_samples is None:
            n_samples = len(self._observations)

        with pyro.plate("person", n_samples):
            # Draw a trait value from Beta. We assume a Gaussian-shaped Beta with mean 0.5 as a prior

            with pyro.plate('traits', 5):
                trait = pyro.sample('trait', dist.Beta(torch.tensor(self.ALPHA_PRIOR), torch.tensor(self.BETA_PRIOR)))

                with pyro.plate("question", 10):                    
                    anwser = pyro.sample('anwser', dist.Binomial(4, trait).mask(anwser_mask)) 

                    return anwser, trait              
            
    def traits_guide(self, n_samples=None, anwser_mask=True):
        if n_samples is None:
            n_samples = len(self._observations)
    
        trait_alphas = pyro.param('trait_alphas', self.ALPHA_PRIOR * torch.ones(5, n_samples), constraint=const.positive)
        trait_betas = pyro.param('trait_betas', self.BETA_PRIOR * torch.ones(5, n_samples), constraint=const.positive)
        
        with pyro.plate("person", n_samples):
            with pyro.plate('traits', 5):
                trait = pyro.sample('trait', dist.Beta(trait_alphas, trait_betas))  

                with pyro.plate("question", 10):                    
                    anwser = pyro.sample('anwser', dist.Binomial(4, trait).mask(anwser_mask)) 

                    return anwser, trait                      
                
    def model_conditioned(self, n_samples=None):               
        observations = self._observations.copy(deep=True)
        observations[NEGATIVE_QUESTIONS] = observations[NEGATIVE_QUESTIONS].apply(lambda x: 4.0 - x if x is not None else None)
        observations = observations.fillna(0.0)

        def model_masked(n_samples = n_samples):
            return self.model(
                n_samples = n_samples,
                anwser_mask = torch.tensor(self.anwser_mask().astype(np.int).reshape(-1, 5, 10).transpose([2, 1, 0]))
            )

        return pyro.condition(model_masked, data = {
            'anwser': torch.tensor(
                observations.drop('country', axis=1).values.reshape(-1, 5, 10).transpose([2, 1, 0])
            )
        })
    
    def infer(self, num_steps = 1_000):    
        model_conditioned = self.model_conditioned()

        pyro.clear_param_store()
        pyro.enable_validation(True)
        svi = pyro.infer.SVI(model=model_conditioned,
                             guide=self.traits_guide,
                             optim=pyro.optim.Adam({"lr": 1e-2}),
                             loss=pyro.infer.TraceGraph_ELBO())

        losses = []
        for t in tqdm(range(num_steps)):
            losses.append(svi.step())

        plt.plot(losses)
        plt.title("ELBO")
        plt.xlabel("step")
        plt.ylabel("loss")
        
    def sample(self, n_samples):
        return pyro.infer.Predictive(self.model, guide=self.traits_guide, num_samples=n_samples)()

    def log_prob(self, data):
        self.reset_observations()
        self.observe(data=data)
        self.infer(3_000)

        model_conditioned = self.model_conditioned()

        guide_trace = poutine.trace(self.traits_guide).get_trace()
        model_trace = poutine.trace(
            poutine.replay(model_conditioned, trace=guide_trace)
        )
        
        return (model_trace.get_trace().log_prob_sum(lambda x,y: x is 'anwser')).detach().numpy()

    def predict_missing(self, data):
        self.reset_observations()
        self.observe(data=data)
        self.infer(2000)

        data = data.copy(deep=True)

        results = stats.mode(self.sample(7)['anwser'].detach().numpy(), axis=0)[0].reshape(50, -1) + 1.0
        results = results.transpose(1, 0)

        mask = self.anwser_mask().astype(np.int) 

        # Flip negational questions
        for i, key in enumerate(QUESTIONS.keys()):
            if key in NEGATIVE_QUESTIONS:
                results[i, :] = 6 - results[i, :]

        # Force conditioned
        data.iloc[:, :50] = data.iloc[:, :50].fillna(0)
        data.iloc[:, :50] = data.iloc[:, :50].values * mask + results * (-mask + 1.0)

        return data
